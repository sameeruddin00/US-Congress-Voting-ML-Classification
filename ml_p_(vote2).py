# -*- coding: utf-8 -*-
"""ML-P-(vote2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAsU-Yvlc4t8fe1GJ2uWUYV9kPc1vQdL
"""

from scipy.io import arff
import pandas as pd

# Load the dataset
file_path = '/content/vote.arff'  # Path to the uploaded dataset
data = arff.loadarff(file_path)
df = pd.DataFrame(data[0])

# Display basic information about the dataset
df_info = {
    "Shape": df.shape,
    "Column Names": df.columns.tolist(),
    "First Few Rows": df.head(),
    "Data Types": df.dtypes,
    "Missing Values": df.isnull().sum()
}

df_info

import numpy as np

# Decode byte strings to regular strings and replace '?' with NaN
df = df.applymap(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
df.replace('?', np.nan, inplace=True)

# Check the updated dataset for missing values
missing_values_summary = df.isnull().sum()

# Display first few rows of the preprocessed dataset
preprocessed_sample = df.head()

missing_values_summary, preprocessed_sample

# Handle missing values
# Impute categorical columns with the mode (most frequent value)
categorical_columns = df.columns[df.dtypes == 'object']
for col in categorical_columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Verify that there are no more missing values
missing_values_after_imputation = df.isnull().sum()

missing_values_after_imputation

from sklearn.preprocessing import LabelEncoder

# Encode categorical variables
encoder = LabelEncoder()
for col in df.columns:
    df[col] = encoder.fit_transform(df[col])

# Display the first few rows of the encoded dataset
encoded_sample = df.head()
encoded_sample

# Display mapping for each column
for col in df.columns:
    print(f"Mappings for column '{col}':")
    unique_values = df[col].unique()
    mapping = {val: encoder.inverse_transform([val])[0] for val in unique_values}
    print(mapping)
    print()

from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop(columns=["Class"])  # Features
y = df["Class"]                 # Target variable

# Split the data (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Display the shapes of the resulting datasets
print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training target shape:", y_train.shape)
print("Testing target shape:", y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import CategoricalNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize the classifiers
logreg = LogisticRegression(max_iter=1000)
naive_bayes = CategoricalNB()
decision_tree = DecisionTreeClassifier()

# Train and evaluate each classifier

# Logistic Regression
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
print("Logistic Regression:")
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))
print(confusion_matrix(y_test, y_pred_logreg))
print()

# Naive Bayes
naive_bayes.fit(X_train, y_train)
y_pred_nb = naive_bayes.predict(X_test)
print("Naive Bayes:")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))
print(confusion_matrix(y_test, y_pred_nb))
print()

# Decision Tree
decision_tree.fit(X_train, y_train)
y_pred_dt = decision_tree.predict(X_test)
print("Decision Tree:")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Set up plotting for confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Logistic Regression Confusion Matrix
cm_logreg = confusion_matrix(y_test, y_pred_logreg)
disp_logreg = ConfusionMatrixDisplay(confusion_matrix=cm_logreg)
disp_logreg.plot(cmap="Blues", ax=axes[0])
axes[0].set_title("Logistic Regression Confusion Matrix")

# Naive Bayes Confusion Matrix
cm_nb = confusion_matrix(y_test, y_pred_nb)
disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb)
disp_nb.plot(cmap="Blues", ax=axes[1])
axes[1].set_title("Naive Bayes Confusion Matrix")

# Decision Tree Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt)
disp_dt.plot(cmap="Blues", ax=axes[2])
axes[2].set_title("Decision Tree Confusion Matrix")

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Metrics for comparison
models = ['Logistic Regression', 'Naive Bayes', 'Decision Tree']
accuracies = [accuracy_score(y_test, y_pred_logreg), accuracy_score(y_test, y_pred_nb), accuracy_score(y_test, y_pred_dt)]
precision_0 = [classification_report(y_test, y_pred_logreg, output_dict=True)['0']['precision'],
               classification_report(y_test, y_pred_nb, output_dict=True)['0']['precision'],
               classification_report(y_test, y_pred_dt, output_dict=True)['0']['precision']]
precision_1 = [classification_report(y_test, y_pred_logreg, output_dict=True)['1']['precision'],
               classification_report(y_test, y_pred_nb, output_dict=True)['1']['precision'],
               classification_report(y_test, y_pred_dt, output_dict=True)['1']['precision']]
recall_0 = [classification_report(y_test, y_pred_logreg, output_dict=True)['0']['recall'],
            classification_report(y_test, y_pred_nb, output_dict=True)['0']['recall'],
            classification_report(y_test, y_pred_dt, output_dict=True)['0']['recall']]
recall_1 = [classification_report(y_test, y_pred_logreg, output_dict=True)['1']['recall'],
            classification_report(y_test, y_pred_nb, output_dict=True)['1']['recall'],
            classification_report(y_test, y_pred_dt, output_dict=True)['1']['recall']]
f1_0 = [classification_report(y_test, y_pred_logreg, output_dict=True)['0']['f1-score'],
        classification_report(y_test, y_pred_nb, output_dict=True)['0']['f1-score'],
        classification_report(y_test, y_pred_dt, output_dict=True)['0']['f1-score']]
f1_1 = [classification_report(y_test, y_pred_logreg, output_dict=True)['1']['f1-score'],
        classification_report(y_test, y_pred_nb, output_dict=True)['1']['f1-score'],
        classification_report(y_test, y_pred_dt, output_dict=True)['1']['f1-score']]

# Plot comparison
fig, ax = plt.subplots(2, 2, figsize=(14, 10))

# Accuracy Comparison
ax[0, 0].bar(models, accuracies, color=['skyblue', 'lightgreen', 'salmon'])
ax[0, 0].set_title("Accuracy Comparison")
ax[0, 0].set_ylabel("Accuracy")

# Precision Comparison for both classes
x = np.arange(len(models))
width = 0.35
ax[0, 1].bar(x - width/2, precision_0, width, label='Democrat Precision', color='lightblue')
ax[0, 1].bar(x + width/2, precision_1, width, label='Republican Precision', color='lightcoral')
ax[0, 1].set_title("Precision Comparison")
ax[0, 1].set_ylabel("Precision")
ax[0, 1].set_xticks(x)
ax[0, 1].set_xticklabels(models)
ax[0, 1].legend()

# Recall Comparison for both classes
ax[1, 0].bar(x - width/2, recall_0, width, label='Democrat Recall', color='lightblue')
ax[1, 0].bar(x + width/2, recall_1, width, label='Republican Recall', color='lightcoral')
ax[1, 0].set_title("Recall Comparison")
ax[1, 0].set_ylabel("Recall")
ax[1, 0].set_xticks(x)
ax[1, 0].set_xticklabels(models)
ax[1, 0].legend()

# F1-score Comparison for both classes
ax[1, 1].bar(x - width/2, f1_0, width, label='Democrat F1', color='lightblue')
ax[1, 1].bar(x + width/2, f1_1, width, label='Republican F1', color='lightcoral')
ax[1, 1].set_title("F1-Score Comparison")
ax[1, 1].set_ylabel("F1-Score")
ax[1, 1].set_xticks(x)
ax[1, 1].set_xticklabels(models)
ax[1, 1].legend()

plt.tight_layout()
plt.show()

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

# Initialize cross-validation strategy (StratifiedKFold ensures balanced splits)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Example for Logistic Regression
logreg_scores = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=cv, scoring='accuracy')
print("Logistic Regression Cross-Validation Scores:", logreg_scores)
print("Mean Accuracy:", logreg_scores.mean())

# You can repeat this for Naive Bayes and Decision Tree
naive_bayes_scores = cross_val_score(CategoricalNB(), X, y, cv=cv, scoring='accuracy')
decision_tree_scores = cross_val_score(DecisionTreeClassifier(), X, y, cv=cv, scoring='accuracy')

# Print mean accuracy for each
print("Naive Bayes Cross-Validation Mean Accuracy:", naive_bayes_scores.mean())
print("Decision Tree Cross-Validation Mean Accuracy:", decision_tree_scores.mean())